\input{preamble.tex}

\title{\huge{ELE201 Semester Project}}
\author{\LARGE{Thobias Høivik}}
\date{}

\begin{document}
\maketitle

\newpage
\tableofcontents

\newpage
\section{Introduction}
In this project we use the STM32 microcontroller along with analog 
sensors 
to gather data that we analyze for statistical and informational 
randomness, 
with the goal of evaluating how feasible such a setup is as a 
hardware-based true random number generator (TRNG). 
The data is sent over 
a network to a server for processing.

We will combine theory from information theory, probability, 
signal processing, and networking to assess both the quality of 
randomness 
and how such a system could integrate into a distributed architecture.

We will discuss:
\begin{itemize}
    \item Theoretical background on randomness and entropy.
    \item Methods for sampling analog noise using microcontrollers.
    \item Implementation and data transfer.
    \item Statistical analysis and randomness testing.
    \item Viability discussion and possible improvements.
    \item Network design for scaling such systems.
\end{itemize}

\newpage
\section{Theory}
\subsection{Randomness and Entropy}
Randomness can be defined both in a statistical and algorithmic sense. 
A sequence is considered random if it is unpredictable and lacks 
compressible structure.

\begin{defn}{Shannon Entropy}{}
Given a discrete random variable $X$ that takes values in $\chi$ with 
probability distribution $p:\chi \to [0,1]$, its entropy is
$$
    H(X) = - \sum_{x \in \chi} p(x) \log_2 p(x).
$$
\end{defn}

Entropy measures the uncertainty of a random variable. 
For a binary sequence, $H(X) = 1$ indicates perfectly balanced and 
maximally unpredictable bits.

Intuitively, high entrop should mean unpredictable bits, while 
low entropy should mean that the bits are structured and predictable.
This sounds very good for our purposes, but if we look at 
a string like "$010101010101\dots$", where we alternate 
$0$s and $1$s, we get a shannon entropy of $1$ which is as high 
as we can go. However, this string of bits very clearly has 
a predictable structure, which entropy alone cannot take into account.

Thus we must find more meassures of randomness. Kolmogorov complexity 

(https://www.cs.cmu.edu/~venkatg/teaching/15252-sp20/notes/Kolmogorov-Complexity.pdf), 

which determines, 
for a piece of data, the smallest program that can produce that 
output. Equivalently, it determines how "compressible" data is.
Take, for instance, the Mandelbrot set which contains an infinite 
amount of points (obviously finite in an image of it), but 
it can be generated by a relatively small program. 

Furthermore if we look at strings, the string 
"$0101010101\dots$", which we discussed earlier, gets a very low 
Kolmogorov complexity as it is just the string $01$ however 
many times we need to get back the string. 

This sounds great! Kolmogorov complexity seems a powerful meassure 
for determining the randomness of our data. However, while the 
repeating sequence of $01$s had a very obvious and easy-to-find 
pattern, what about a string like 
"$011010111011001010111110100010$"? No immediate pattern jumps out 
like it did in the example above. There probably is a pattern, but 
what if this string was $100x$ longer with other patterns 
in it? As it turns out, determining the Kolmogorov complexity 
of a piece of data is very hard. Not only is it very hard, but it 
is in fact undecidable in general.

NOTE: Possible useful defns for l8r $\rightarrow$


\begin{defn}{Entropy Rate}{}
    The entropy rate of a stochastic process $(X_n)$ is defined as
    $$
    h(X) = \lim_{n \to \infty} \frac{1}{n} H(X_1, X_2, \dots, X_n),
    $$
    which measures the average uncertainty contributed by each new observation.
\end{defn}

\begin{defn}{Conditional Entropy}{}
For two random variables $X$ and $Y$,
$$
H(X|Y) = -\sum_{x,y} p(x,y) \log_2 p(x|y),
$$
representing the expected uncertainty in $X$ given that $Y$ is known.
\end{defn}

\begin{defn}{Min-Entropy}{}
The min-entropy of a random variable $X$ is
$$
H_{\text{min}}(X) = -\log_2 \left(\max_x p(x)\right),
$$
representing the worst-case predictability of any outcome.
\end{defn}

\subsection{Sources of Entropy in Hardware}
Microcontrollers can extract entropy from physical phenomena such as:
\begin{itemize}
    \item Thermal noise (Johnson–Nyquist noise) in resistors or 
        sensors.
    \item Electronic jitter in oscillators and ADCs.
    \item Fluctuations in analog sensors (light, temperature, sound, 
        etc.).
\end{itemize}
These sources are often amplified and sampled via the ADC to produce 
random bits.

\subsection{True vs. Pseudo Randomness}
A pseudo-random number generator (PRNG) produces deterministic 
sequences 
from an initial seed, while a true random number generator (TRNG) 
relies 
on physical entropy. 
For hardware-based RNGs, ensuring unbiased and unpredictable output 
requires:
\begin{itemize}
    \item High-quality analog entropy sources.
    \item Proper sampling rates and whitening.
    \item Statistical post-processing.
\end{itemize}

\subsection{Statistical Tests of Randomness}
We use the NIST SP 800-22 statistical test suite as a theoretical 
foundation. The most relevant tests include:
\begin{itemize}
    \item Frequency (Monobit) Test
    \item Runs Test
    \item Autocorrelation Test
    \item Approximate Entropy Test
    \item Linear Complexity Test
\end{itemize}
Each of these tests returns a $p$-value indicating how likely the 
observed sequence could occur under true randomness.

\subsection{Compression and Algorithmic Randomness}
Kolmogorov complexity describes how compressible a sequence is. 
In practice, this can be approximated by compressing the data using 
a general-purpose algorithm (e.g., gzip) and comparing the compression ratio.

\subsection{Signal Processing Perspective}
Entropy in analog signals can also be characterized by:
\begin{itemize}
    \item Autocorrelation function (should approach 0 for nonzero lag).
    \item Power spectral density (should be approximately flat for white noise).
\end{itemize}

\newpage
\section{Implementation}
\subsection{Hardware Setup}
We will use the STM32 microcontroller, connected to one or more analog 
sensors. Candidate sensors include:
\begin{itemize}
    \item Microphone or sound sensor.
    \item Light-dependent resistor (LDR).
    \item Temperature sensor.
\end{itemize}

The microcontroller samples the analog voltage via the ADC and 
stores or 
streams the digital values over a serial or network interface.

\subsection{Data Transmission}
Data is transmitted from the STM32 to a computer for analysis. 
This can be done using:
\begin{itemize}
    \item UART/Serial connection.
    \item Ethernet or WiFi (via ESP module or similar).
\end{itemize}

We use a lightweight protocol to ensure each sample is timestamped 
and properly framed. The receiving server stores the data in a 
binary file.

\subsection{Software Stack}
On the analysis computer:
\begin{itemize}
    \item Python scripts for entropy calculation and statistical tests.
    \item Optional C or Rust implementation for efficient batch 
        processing.
    \item Visualization of autocorrelation and frequency distribution.
\end{itemize}

\subsection{Data Preprocessing}
Collected data may exhibit bias or patterns due to hardware noise or 
ADC quantization. We can apply:
\begin{itemize}
    \item Mean removal and normalization.
    \item Bit extraction (e.g., least significant bit of ADC samples).
    \item Von Neumann debiasing if needed.
\end{itemize}

\newpage
\section{Results and Analysis}
\subsection{Entropy Estimates}
We calculate Shannon entropy for bit sequences of various lengths, 
expecting values close to 1 for highly random data.

\subsection{Frequency and Runs Tests}
We evaluate balance between 0s and 1s and the expected number of 
alternations. 

\subsection{Autocorrelation and Spectral Tests}
We compute autocorrelation functions and Fourier transforms to look 
for repeating structures or periodic interference.

\subsection{Compression Ratio}
We compress the data using a lossless compressor to check for 
compressibility. A ratio near 1 indicates high randomness.

\subsection{Discussion of Findings}
We discuss:
\begin{itemize}
    \item Whether the observed randomness is sufficient for 
        cryptographic use.
    \item How sensor noise compares to other hardware RNGs.
    \item Effects of sampling rate and environmental conditions.
\end{itemize}

\newpage
\section{Network Architecture and Subnetting}
\subsection{System Overview}
We consider a distributed network of sensor-based TRNG nodes, each 
sending entropy samples to a central collector.

\subsection{Network Design}
A possible setup:
\begin{itemize}
    \item Each microcontroller is connected to a local router.
    \item A dedicated subnet for sensor nodes (e.g., 192.168.10.0/24).
    \item Central analysis server on a separate subnet.
\end{itemize}

\subsection{Subnetting and Addressing}
We show how to subnet the network efficiently:
\begin{itemize}
    \item Example: dividing a /24 into four /26 subnets.
    \item Assigning IP ranges for sensors, analysis nodes, and administration.
\end{itemize}

\subsection{Data Security and Transmission Integrity}
We briefly discuss:
\begin{itemize}
    \item Packet integrity verification (CRC or checksum).
    \item Optional encryption for data in transit.
    \item Synchronization and time-stamping for accurate sampling.
\end{itemize}

\newpage
\section{Conclusion}
We summarize:
\begin{itemize}
    \item Theoretical feasibility of analog-sensor-based TRNGs.
    \item Experimental results and limitations.
    \item Potential for distributed entropy networks.
\end{itemize}

\subsection{Future Work}
\begin{itemize}
    \item Hardware whitening circuits and amplification.
    \item FPGA or ASIC implementations.
    \item Scaling to larger networks and entropy pooling.
\end{itemize}

\newpage
\section*{References}
(Include IEEE papers, arXiv articles, and videos cited earlier.)
\end{document}

