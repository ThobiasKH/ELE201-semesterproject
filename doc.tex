\input{preamble.tex}

\title{\huge{ELE201 Semester Project}}
\author{\LARGE{Thobias HÃ¸ivik}}
\date{}

\begin{document}
\maketitle

\newpage
\tableofcontents

\newpage
\section{Introduction}
In this project we use the STM32 microcontroller along with analog 
sensors 
to gather data that we analyze for statistical and informational 
randomness, 
with the goal of evaluating how feasible such a setup is as a 
hardware-based true random number generator (TRNG). 
The data will be sent via serial connection to a computer 
to analyze the quality of the data.

We will combine theory from information theory, probability, 
signal processing, and networking to assess both the quality of 
randomness 
and how such a system could integrate into a distributed architecture.

We will discuss:
\begin{itemize}
    \item Theoretical background on randomness and entropy.
    \item Implementation and data transfer.
    \item Statistical analysis and randomness testing.
    \item Viability discussion and possible improvements.
    \item Network design for scaling such systems.
\end{itemize}

\newpage
\section{Theory}
\subsection{Randomness and Entropy}
Randomness can be defined both in a statistical and algorithmic sense. 
A sequence is considered random if it is unpredictable and lacks 
compressible structure.

\begin{defn}{Shannon Entropy}{}
Given a discrete random variable $X$ that takes values in $\chi$ with 
probability distribution $p:\chi \to [0,1]$, its entropy is
$$
    H(X) = - \sum_{x \in \chi} p(x) \log_2 p(x).
$$
\end{defn}

Entropy measures the uncertainty of a random variable. 
For a binary sequence, $H(X) = 1$ indicates perfectly balanced and 
maximally unpredictable bits.

Intuitively, high entrop should mean unpredictable bits, while 
low entropy should mean that the bits are structured and predictable.
This sounds very good for our purposes, but if we look at 
a string like "$010101010101\dots$", where we alternate 
$0$s and $1$s, we get a shannon entropy of $1$ which is as high 
as we can go. However, this string of bits very clearly has 
a predictable structure, which entropy alone cannot take into account.

\begin{defn}{Min-Entropy}{}
The min-entropy of a random variable $X$ is
$$
H_{\text{min}}(X) = -\log_2 \left(\max_x p(x)\right),
$$
representing the worst-case predictability of any outcome.
\end{defn}

\subsection{True vs. Pseudo Randomness}
A pseudo-random number generator (PRNG) produces deterministic 
sequences 
from an initial seed, while a true random number generator (TRNG) 
relies 
on physical entropy. 
For hardware-based RNGs, ensuring unbiased and unpredictable output 
requires:
\begin{itemize}
    \item High-quality analog entropy sources.
    \item Proper sampling rates and whitening.
    \item Statistical post-processing.
\end{itemize}

\subsection{Statistical Tests of Randomness}
We use the NIST SP 800-22 statistical test suite as a theoretical 
foundation. The most relevant tests include:
\begin{itemize}
    \item Frequency (Monobit) Test
    \item Runs Test
    \item Autocorrelation Test
    \item Approximate Entropy Test
    \item Linear Complexity Test
\end{itemize}
Each of these tests returns a $p$-value indicating how likely the 
observed sequence could occur under true randomness.

\subsection{Tentative Predictions}

\newpage
\section{Implementation}
\subsection{Hardware Setup}
We will use the STM32 microcontroller, connected to one or more analog 
sensors. Candidate sensors include:
\begin{itemize}
    \item Microphone or sound sensor.
    \item Light-dependent resistor (LDR).
    \item Temperature sensor.
\end{itemize}

The microcontroller samples the analog voltage via the ADC and 
stores or 
streams the digital values over a serial or network interface.
For now we will be working with a light sensor, sampling 
at $12$ bits and looking at bits in the lower end of the 
range as they will be most prone to interference and noise.

\subsection{Data Transmission}
Data is transmitted from the STM32 to a computer for analysis. 
This can be done using:
\begin{itemize}
    \item UART/Serial connection.
    \item Ethernet or WiFi (via ESP module or similar).
\end{itemize}

For our purpose of analyzing the data for randomness a serial 
connection will do and we will use UART, sending the data 
to a computer where we can analyze the data.

\subsection{Processing}
Our goal is to transform the raw data stream 
(which is \textbf{biased} and \textbf{predictable}) into a shorter, 
statistically perfect random stream.

\begin{enumerate}
    \item Raw Bit Extraction 
        \begin{itemize}
            \item \textbf{Technique:} Only use the least significant
                bits of the ADC output. 
            \item The LSBs are dominated by the desired random noise 
                sources and are less influenced by the much larger 
                and more predictable signal 
                (e.g. light level in the case of a light sensor).
                Thus the MSBs should be discarded entirely as they 
                should exibit fairly low entropy.
        \end{itemize}

    \item Entropy Conditioning (Post-Processing) 
        Since the LSBs will more than likely still contain 
        residual bias and correlation we require a powerful 
        \textbf{Entropy Extractor} for cryptographic quality.
        \begin{itemize}
            \item \textbf{XOR Folding:}
                XOR a bit with one a few steps behind. 
                E.g. $\text{Bit}_{\text{Out}} = 
                \text{Bit}_i \oplus \text{Bit}_{i-N}$, 
                where $N$ is chosen to be slightly larger than 
                the observed correlation length.

            \item \textbf{Cryptographic Hash Extractor 
                (NIST SP 800-90B):}
                Collect a large buffer of $L$ raw bits. 
                Estimate $H_{\text{min}}$ per bit. Take the buffer, 
                and hash it (e.g. SHA-256) to produce a fixed-length 
                output.

                Ths downside with this approach is that we have to 
                sample many times to get a sufficiently large buffer.
        \end{itemize}
\end{enumerate}



\newpage
\section{Results and Analysis}

\subsection{Network Design}
A possible setup:
\begin{itemize}
    \item Each microcontroller is connected to a local router.
    \item A dedicated subnet for sensor nodes (e.g., 192.168.10.0/24).
    \item Central analysis server on a separate subnet.
\end{itemize}

\subsection{Subnetting and Addressing}
We show how to subnet the network efficiently:
\begin{itemize}
    \item Example: dividing a /24 into four /26 subnets.
    \item Assigning IP ranges for sensors, analysis nodes, and administration.
\end{itemize}

\subsection{Data Security and Transmission Integrity}
We briefly discuss:
\begin{itemize}
    \item Packet integrity verification (CRC or checksum).
    \item Optional encryption for data in transit.
    \item Synchronization and time-stamping for accurate sampling.
\end{itemize}

\newpage
\section{Conclusion}
We summarize:
\begin{itemize}
    \item Theoretical feasibility of analog-sensor-based TRNGs.
    \item Experimental results and limitations.
    \item Potential for distributed entropy networks.
\end{itemize}

\subsection{Future Work}
\begin{itemize}
    \item Hardware whitening circuits and amplification.
    \item FPGA or ASIC implementations.
    \item Scaling to larger networks and entropy pooling.
\end{itemize}

\newpage
\section*{References}
(Include IEEE papers, arXiv articles, and videos cited earlier.)
\end{document}

