\input{preamble.tex}
\usepackage{graphicx}



\title{\huge{Semester Project}}
\author{\LARGE{Thobias Høivik} 
    \\ \large{Western Norway University of Applied Sciences} 
\\ \large{ELE201: Microcontrollers and Data Networks}}
\date{}

\begin{document}
\maketitle

\newpage
\tableofcontents

\newpage
\section{Introduction}
In this project we use the STM32 microcontroller along with analog 
sensors 
to gather data that we analyze for statistical and informational 
randomness, 
with the goal of evaluating how feasible such a setup is as a 
hardware-based true random number generator (TRNG). 
The data will be sent via serial connection to a computer 
to analyze the quality of the data.

We will combine theory from information theory, probability, 
signal processing, and networking to assess both the quality of 
randomness 
and how such a system could integrate into a distributed architecture.

We will discuss:
\begin{itemize}
    \item Theoretical background on randomness and entropy.
    \item Implementation and data transfer.
    \item Statistical analysis and randomness testing.
    \item Viability discussion and possible improvements.
    \item Network design for scaling such systems.
\end{itemize}

\newpage
\section{Theory}
\subsection{Randomness and Entropy}
Randomness can be defined both in a statistical and algorithmic sense. 
A sequence is considered random if it is unpredictable and lacks 
compressible structure.

\begin{defn}{Shannon Entropy}{}
Given a discrete random variable $X$ that takes values in $\chi$ with 
probability distribution $p:\chi \to [0,1]$, its entropy is
$$
    H(X) = - \sum_{x \in \chi} p(x) \log_2 p(x).
$$
\end{defn}
(as introduced by Shannon \cite{Shannon1948} 
and discussed in \cite{Gray1990,StatisticShowTo}).
Entropy is a central meassure in information theory pertaining 
to the randomness of data and it will be referenced extensively 
throughout this text.

\subsection{True vs. Pseudo Randomness}
A pseudo-random number generator (PRNG) produces deterministic 
sequences 
from an initial seed, while a true random number generator (TRNG) 
relies 
on physical entropy \cite{HardwareRNGOverview}. 
For hardware-based RNGs, ensuring unbiased and unpredictable output 
requires:
\begin{itemize}
    \item High-quality analog entropy sources.
    \item Statistical post-processing.
\end{itemize}

\subsection{Hypothesis}
We believe that due to the inherit noise of electrical circuits 
that dominate at low frequencies, the 
least significant bits (LSBs) of our signal should be a source of 
high entropy. Then, with further post-processing we should 
get a high-quality source of random numbers which can be applied 
in processes which require randomness.

\newpage
\subsection{Theoretical Modeling of Analog Noise} 
To justify that the least significant bits of our ADC contains 
randomness, we model the analog sensor output as a combination of 
deterministic signal and noise: 
\[ 
    V_{\text{ADC}}(t) = V_{\text{Signal}}(t) + V_{\text{Noise}}(t)
\]

\begin{itemize}
    \item \(V_{\text{Signal}}\) represents the slowly varying, 
        predictable component (e.g. ambient light level in the case 
        of photoresistor). 
    \item \(V_{\text{Noise}}\) represents the unpredictable physical 
        noise (thermal-, shot noise, quantization error, 
        sensor-specific noise).
\end{itemize}

The noise, which is the sum of thermal-, shot-, and other electronic 
noise is typically modeled as a Gaussian random variable 
\cite{Gundersen2019}:
\[ 
    V_{\text{Noise}}(t) \sim \mathcal N(0, \sigma^2_{\text{Noise}}) 
\]

The \textbf{ADC quantization} transforms the continuous voltage 
into discrete levels: 
\begin{align*}
    X = \text{ADC}(V_{\text{ADC}}) \in \{0,1,\dots,2^{12}-1\}
\end{align*}

The voltage corresponding to one LSB (least significant bit) step 
is 
\[ 
    \Delta V = \frac{V_{\text{ref}}}{2^{12}}
\]
for a \(12\)-bit ADC with reference voltage \(V_{\text{ref}}\).


\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{./images/quantization.png} 
    \caption{Quantization of analog signal.}
    \label{fig:quantization} 
\end{figure}

The randomness of the extracted bits is meassured by the 
Shannon Entropy. For a discrete random variable \(X\) with 
\(k\) possible outcomes, the maximum possible entropy 
\(H_{\text{MAX}} = \log_2(k)\) bits. 

We extract the \(4\) LSBs, which gives \(2^4 = 16\) possible 
outcomes from \(0000_b \to 1111_b\). The maximum entropy then is 
\[ 
    H_{\text{MAX}} = 4 \text{ bits}
\]

Maximum entropy is achieved if the the probability of 
observing any of the \(16\) outcomes is uniform
\cite{Gray1990,StatisticShowTo}:
\[ 
    P(D_{\text{LSB}} = i) = \frac{1}{16} \text{ for } 0 \leq i \leq 15
\]

To achieve a near-uniform distribution across the \(16\) LSB 
states, the standard deviation of the noise \(\sigma_{\text{Noise}}\)
must be large enough to span multiple quantization steps.

Let \(\Delta V_{4-\text{bit}}\) be the voltage corresponding to 
the \(4\) LSBs: 
\[ 
    \Delta V_{4-\text{bit}} = 16\Delta V
\]

Consider an input voltage \(V_{\text{ADC}}\) that falls within 
a \(\Delta V_{4-\text{bit}}\) window. The probability that 
the resulting digital word \(D\) falls into a specific \(4\)-bit 
LSB state \(i\) depends on the area under the Gaussian Probability 
Density function of the noise that falls into the corresponding 
voltage interval \(I_i\). 

The critical condition for near-maximum entropy is then: 
\[ 
    \sigma_{\text{Noise}} \gg \Delta V
\]

If the noise standard deviation is significantly larger than 
the LSB step size then the Gaussian noise distribution becomes 
"smeared" across multiple LSB intervals. Since the noise is 
zero-mean and \(\sigma_{\text{Noise}}\) is large, the probability 
of the total input \(V_{\text{ADC}}\) falling into any one 
LSB interval \(I_i\) is nearly equal to the probability of 
it falling into an adjacent \(I_{i \pm 1}\). 




\newpage
\section{Implementation}
\subsection{Hardware Setup}
We will use the STM32f767zi microcontroller, connected to a 
photoresistor.

The microcontroller samples the analog voltage via the ADC and 
stores or 
streams the digital values over a serial or network interface.
We will be sampling at \(12\)-bits, keeping the \(4\) least 
significant bits as we believe these will be most susceptible to 
noise.


\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\linewidth]{./images/WIRING_DIAGRAM_LIGHT.png} 
    \caption{Diagram}
    \label{fig:wire_diagram} 
\end{figure}

\subsection{Data Transmission}
Data is transmitted from the STM32 to a computer for analysis. 
In our experimentation we use USART/Serial Connection using 
a micro-usb connection from the microcontroller to a computer.

\newpage
\section{Results and Analysis}
\subsection{Results from the Raw Signal}
We begin by analyzing the apparent randomness 
of the raw signal. 

In our experimentation we first use Python with the serial library 
to read the incoming data and matplotlib to plot the signal 
over time.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{./images/LIVE_READINGS_SMALL_WINDOW.png} 
    \caption{Plot of the raw signal over time}
    \label{fig:raw_signal_plot} 
\end{figure}

As we can see in this plot, while the signal remains relatively stable,
reflecting the stable light level we were testing against, 
there are definite micro-jitters present. This is a good 
sign and is almost ceirtainly due to the noise which we predicted 
would dominate at small levels.

We do have to take into account the fact that the micro-jitters 
might be due to slight fluctuations of the ambient light-level in 
our tests. To mitigate this possibility we will from this time 
forward be testing with the photoresistor covered with electrical 
tape. 

\newpage
Now we look at the observed probabilities pertaining to 
the \(4\) LSBs. We want a roughly equal probability that a given 
bit in the \(4\) LSBs is \(0\) or \(1\).

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{./images/HISTOGRAM_WITH_ENTROPY.png} 
    \caption{Histogram of 4 LSBs with photoresistor covered}
    \label{fig:histogram_LSB_covered}
\end{figure}

As we can see there appears to be a roughly equal chance that a 
given bit in the \(4\) LSBs is \(0\) or \(1\), pointing towards 
high entropy in the LSBs.

Furthermore, we can see that each bit has an entropy of \(1\)
and the sum of the \(4\) LSBs entropies is \(4\). 
As we determined earlier the maximum entropy \(H_{\text{MAX}} = 4\), 
meaning that the \(4\) LSBs exibit maximum entropy.
 
\subsection{Processing}
Our goal is to transform the raw data stream 
(which is \textbf{biased} and \textbf{predictable}) into a shorter, 
statistically perfect random stream.
So far we've already done some processing in discarding all, but 
the \(4\) LSBs. 

As discussed previously, the \(4\) LSBs are a good source of entropy, 
but using \(4\) bits at a time is not sensible as it 
is relatively easy to predict given the small number of combinations 
at only \(16\).

Now, since we have observed that each individual bit of the 
LSBs exibit a maximum entropy (\(1\)), then taking 
\(64\) samples and appending each by bit-shifting 
\(4\) bits to the left should give us a 
\(256\) bit number with maximum entropy.

In this section we will examine \(4\) central metrics in 
determining the quality of the \(256\)-bit number. 
\begin{enumerate}
    \item \textbf{Entropy.} Entropy is still important and 
        we want an entropy as close to \(1\) as possible in 
        this case. 
    \item \textbf{Bit frequency.} We examine how many 0s vs. 1s 
        there are in a given bit-string. A \(50\)/\(50\) 
        distribution is obviously desireable.
    \item \textbf{Autocorrelation.} The correlation between 
        consecutive bits is important and we want 
        to be as close to \(0\) as possible, indicating 
        no correlation/pattern in consecutive bits.
    \item \textbf{Runs.} Runs are the number of "blocks" of like 
        bits. For example \(0111110_b\) contains \(3\) blocks, 
        the first \(0\), the consecutive \(1\)s and the final 
        \(0\). A run of \(256\) is bad because it indicates 
        alternating \(0\)s and \(1\)s. \(1\) is also very bad 
        as it indicates that the string only has \(0\)s or \(1\)s.
        Hence we want a run-count somewhere in the middle of these 
        extremes.
\end{enumerate}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{./images/256SUITE_PLOT.png} 
    \caption{Readings of various metrics in 256-bit number.}
    \label{fig:256_suite}
\end{figure}

Our readings show that our \(256\)-bit numbers tend to 
hover around the desired values, never reaching any 
disconcerting values during the test that produced this 
plot. During this test, \(434\) numbers were analyzed with 
the following averages: 
\begin{enumerate}
    \item Average Entropy: \(0.987\) 
    \item Average Bit Frequency: \(0.472\)
    \item Average Autocorrelation: \(-0.045\) 
    \item Average Runs: \(131.990\)
\end{enumerate}

These results are promising, but we want to employ a more stringent 
test to analyze the \textbf{RNG}-quality of our 
\(256\)-bit blocks.
The employed test suite enforces a rigorous hypothesis testing 
framework, where the \textbf{null hypothesis} $H_0$ posits 
the bitstream 
as being perfectly random, and results are accepted only if the 
calculated P-value is greater than the significance level 
$\alpha=0.01$. Specifically, the \textbf{Monobit Frequency Test} 
assesses the uniformity of $0$s and $1$s by calculating the $Z$-score,
$$ 
    Z = |S_n - n/2| / \sqrt{n/4}
$$
, where $S_n$ is the sum of ones 
in the $n=256$ bit sequence, and deriving the P-value via the 
complementary error function, $P_1 = \text{erfc}(Z/\sqrt{2})$. 
The \textbf{Runs Test} evaluates the occurrence of contiguous 
identical 
bits by calculating the observed number of runs $V_n$ and comparing 
it against the expected mean $\mu$ and variance $\sigma^2$, 
again yielding a $Z$-score 
$$ 
    Z = |V_n - \mu| / \sqrt{\sigma^2} 
$$ 
for the P-value $P_2$. Finally, 
the \textbf{Serial Overlapping Blocks Test} serves as a strong proxy 
for Approximate Entropy by using a $\chi^2$ goodness-of-fit 
statistic 
$$
    \chi^2 = \displaystyle\sum_{i=1}^{16} \frac{(C_i - E)^2}{E}
$$
across all $2^m=16$ possible $m=4$ bit blocks, 
with the P-value $P_3$ determined by the Chi-squared survival 
function $\chi^2.sf(\chi^2, 2^m - 1)$. The block is certified as 
\emph{random} if and only if $P_1 \geq \alpha \land P_2 \geq \alpha 
\land P_3 \geq \alpha$, providing a composite measure highly 
resistant to statistical bias.

With this approach we quickly discover the limitations of 
using only the raw signal of our LDR to create our numbers.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{./images/RNG_TEST_RAW.png} 
    \caption{Results from our test suite}
    \label{fig:256_suite_2}
\end{figure}

It is clear, from the fact that less than half of our \(256\)-bit 
numbers pass all our tests, that some more involved processing 
is required to get high-enough-quality random numbers.

\subsection{Network Design}
A possible setup:
\begin{itemize}
    \item Each microcontroller is connected to a local router.
    \item A dedicated subnet for sensor nodes (e.g., 192.168.10.0/24).
    \item Central analysis server on a separate subnet.
\end{itemize}

\subsection{Subnetting and Addressing}
We show how to subnet the network efficiently:
\begin{itemize}
    \item Example: dividing a /24 into four /26 subnets.
    \item Assigning IP ranges for sensors, analysis nodes, and administration.
\end{itemize}

\subsection{Data Security and Transmission Integrity}
We briefly discuss:
\begin{itemize}
    \item Packet integrity verification (CRC or checksum).
    \item Optional encryption for data in transit.
    \item Synchronization and time-stamping for accurate sampling.
\end{itemize}

\newpage
\section{Conclusion}
We summarize:Fundamentals of Precision ADC
Noise Analysis
\begin{itemize}
    \item Theoretical feasibility of analog-sensor-based TRNGs.
    \item Experimental results and limitations.
    \item Potential for distributed entropy networks.
\end{itemize}

\subsection{Future Work}
\begin{itemize}
    \item Hardware whitening circuits and amplification.
    \item FPGA or ASIC implementations.
    \item Scaling to larger networks and entropy pooling.
\end{itemize}

\newpage
\begin{thebibliography}{99}  
    \raggedright

\bibitem{Shannon1948}
C. E. Shannon,
\textit{A Mathematical Theory of Communication},
Bell System Technical Journal,
vol. 27, pp. 379--423, 623--656, 1948.

\bibitem{Gray1990}
R. M. Gray,
\textit{Entropy and Information Theory},
Springer, 1990.

\bibitem{HardwareRNGOverview}
Cerberus Security,
\textit{Hardware Random Number Generators},
2020.
\url{https://cerberus-laboratories.com/blog/random_number_generators/}

\bibitem{StatisticShowTo}
Statistics How To,
\textit{Shannon Entropy Definition},
\url{https://www.statisticshowto.com/shannon-entropy/}

\bibitem{Gundersen2019}
G. Gundersen,
\textit{Random Noise and the Central Limit Theorem},
Blog post, 01 February 2019.
\url{https://gregorygundersen.com/blog/2019/02/01/clt/}



\end{thebibliography}

\end{document}

